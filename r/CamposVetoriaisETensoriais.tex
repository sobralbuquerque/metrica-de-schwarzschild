\chapter{Campos vetoriais e tensoriais}\label{cap:CamposVetoriaisETensoriais}
\section{Sistemas de coordenadas no espaço euclidiano}\label{sec:SistemasCoordenadasEspacoEuclidiano}
Considerando um espaço euclidiano tridimensional equipado com um sistema cartesiano de coordenadas $ (x,y,z) $ e um conjunto de vetores associados $ \{\mathbf{i}, \mathbf{j}, \mathbf{k}\} $. Seja um outro sistema de coordenadas $ (u,v,w) $ não-cartesiano. Nós podemos expressar as coordenadas cartesianas em termos destas:

\begin{equation}\label{eq:TransformacaoXU}
	x=x(u, v, w), \quad y=y(u, v, w), \quad z=z(u, v, w)
\end{equation}
	
E, à princípio, inverter as relações e escrever $ u,v,w $ em termos de $ x,y,z $. A equação \eqref{eq:TransformacaoXU} em notação vetorial fica:

\begin{equation}\label{eq:TransformacaoXUVetorial}
\mathbf{r}=x(u, v, w) \mathbf{i}+y(u, v, w) \mathbf{j}+z(u, v, w) \mathbf{k}
\end{equation}

Agora, pode-se fixar uma das coordenadas, chegando em uma superfície parametrizada pelas outras duas. Por exemplo, fazendo-se $ w=w_0 $, temos a superfície coordenada $ \mathbf{r}=x\left(u, v, w_{0}\right) \mathbf{i}+y\left(u, v, w_{0}\right) \mathbf{j}+z\left(u, v, w_{0}\right) \mathbf{k} $ parametrizada por $ u,v $ e analogamente para as outras duas.

Fixando-se duas coordenadas (e.g. $ v=v_0,w=w_0 $), tem-se uma curva coordenada parametrizada em $ u $ (neste caso, dada pela interseção das superfícies coordenadas $ v=v_0 $ e $ w=w_0 $).

\begin{equation}\label{eq:CurvaCoordenadaU}
 \mathbf{r}=x\left(u, v_{0}, w_{0}\right) \mathbf{i}+y\left(u, v_{0}, w_{0}\right) \mathbf{j}+z\left(u, v_{0}, w_{0}\right) \mathbf{k} 
\end{equation}

As outras curvas coordenadas são geradas da mesma maneira. Derivando-se a equação \eqref{eq:CurvaCoordenadaU} em relação ao parâmetro $ u $, tem-se o vetor tangente à curva coordenada. Mas isso é igual a derivar parcialmente em $ u $ a equação \eqref{eq:TransformacaoXUVetorial}. Assim, os vetores tangentes às curvas coordenadas que passam por $ P=(u_0,v_0,w_0) $ são


\begin{equation}\label{eq:VetoresCoordenadosU}
\boxed{\mathbf{e}_{\mathfrak{u}} \equiv \partial \mathbf{r} / \partial u, \quad \mathbf{e}_{v} \equiv \partial \mathbf{r} / \partial v, \quad \mathbf{e}_{w} \equiv \partial \mathbf{r} / \partial w}
\end{equation}


Com as derivadas tomadas em $ (u_0,v_0,w_0) $.

Sejam 

\[
h_{1} \equiv\left|\mathbf{e}_{u}\right|, \quad h_{2} \equiv\left|\mathbf{e}_{v}\right|, \quad h_{3} \equiv\left|\mathbf{e}_{w}\right|
\]

Podem-se normalizar os vetores:

\[
\hat{\mathbf{e}}_{u}=\frac{1}{h_{1}} \mathbf{e}_{u}, \quad \mathbf{\hat { e }}_{v}=\frac{1}{h_{2}} \mathbf{e}_{v}, \quad \mathbf{\hat { e }}_{w}=\frac{1}{h_{3}} \mathbf{e}_{w}
\]

O conjunto $ \left\{\hat{\mathbf{e}}_{\boldsymbol{u}}, \mathbf{\hat { e }}_{\boldsymbol{v}}, \hat{\mathbf{e}}_{\boldsymbol{w}}\right\} $ forma uma base em $ P $ e, assim, podemos escrever qualquer vetor $ \mathbf{\lambda} $ na forma

\[
\boldsymbol{\lambda}=\alpha \hat{\mathbf{e}}_{u}+\beta \hat{\mathbf{e}}_{v}+\gamma \hat{\mathbf{e}}_{w}
\]
A terna $ \alpha,\beta,\gamma $ compõe as coordenadas nessa base.

Outro modo de se criar uma base é com a normal das superfícies coordenadas. Invertendo as relações 
\eqref{eq:TransformacaoXU}, temos

\begin{equation}\label{eq:TransformacaoUX}
u=u(x, y, z), \quad v=v(x, y, z), \quad w=w(x, y, z)
\end{equation}

Assim, podemos trabalhar com cada coordenada como um campo escalar e calcular seu gradiente:

\begin{equation}\label{eq:GradienteTransformacaoUX}
\begin{aligned} \nabla u &=\frac{\partial u}{\partial x} \mathbf{i}+\frac{\partial u}{\partial y} \mathbf{j}+\frac{\partial u}{\partial z} \mathbf{k} \\ \nabla v &=\frac{\partial v}{\partial x} \mathbf{i}+\frac{\partial v}{\partial y} \mathbf{j}+\frac{\partial v}{\partial z} \mathbf{k} \\ \nabla w &=\frac{\partial w}{\partial x} \mathbf{i}+\frac{\partial w}{\partial y} \mathbf{j}+\frac{\partial w}{\partial z} \mathbf{k} \end{aligned}
\end{equation}

A cada ponto $ P $, esses vetores são normais às superfícies coordenadas correspondentes, que são $ u=u_0, v=v_0, w=w_0 $. Assim, eles formam uma base alternativa em $ P $. Chamamo-na de base \textit{dual}. Para diferenciá-la da obtida anteriormente, os sufixos são sobrescritos:

\begin{equation}\label{eq:VetoresDuaisU}
\boxed{\mathbf{e}^{u} \equiv \nabla u, \quad \mathbf{e}^{v} \equiv \nabla v, \quad \mathbf{e}^{w} \equiv \nabla w}
\end{equation} 

Dado um campo vetorial $ \mathbf{\lambda} $, é possível escrevê-lo em ambas as bases:

\begin{equation}\label{eq:LambdaEmAmbasAsBases}
\begin{array}{l}{\boldsymbol{\lambda}=\lambda^{u} \mathbf{e}_{u}+\lambda^{v} \mathbf{e}_{v}+\lambda^{w} \mathbf{e}_{w}} \\ {\boldsymbol{\lambda}=\lambda_{u} \mathbf{e}^{u}+\lambda_{v} \mathbf{e}^{v}+\lambda_{w} \mathbf{e}^{w}}\end{array}
\end{equation}

\section{Notação de sufixos}\label{sec:NotacaoSufixo}
A notação de sufixos nos dá uma maneira de tratar uma coleção de quantidades relacionadas, tais como as coordenadas de um ponto ou um vetor. A ideia básica é representar os elementos dessa coleção com uma mesma letra base na qual se adiciona um sufixo, de modo a rotular a quantidade da coleção. Podem-se utilizar sufixos tanto subscritos quanto sobrescritos, além de poder haver mais de um sufixo anexado à letra base. Iremos utilizá-lo, junto a convenção de somatório, de modo a compactar o tratamento das quantidades em que estamos interessados.

Primeiramente, vamos convencionar que sufixos simbolizados por letras minúsculas no meio do alfabeto (i.e., $i, j, k, \ldots$) sempre variam ao longo dos valores $1, 2, 3$, de modo que não precisamos escrever um comentário em parêntesis $(i=1,2,3)$ sempre que utilizarmos um índice. Além disso, utilizaremos a convenção de que um índice representado por uma letra minúscula no meio do alfabeto ($a,b,c,\ldots$) varia em $1,2,\ldots,N$; letras maíusculas no inicio do alfabeto ($A,B,C\ldots$) vão de $1,2$ e; letras do alfabeto grego ($\mu,\nu,\sigma,\ldots$) variam ao longo de $0,1,2,3$.

Assim, utilizaremos $u^i$ em vez de $(u,v,w)$ para as coordenadas, $\left\{\mathbf{e}_{i}\right\}$ em vez de $\left\{\mathbf{e}_{u}, \mathbf{e}_{v}, \mathbf{e}_{w}\right\}$ para a base natural e $\left\{\mathbf{e}^{i}\right\}$ em vez de $\left\{\mathbf{e}^{u}, \mathbf{e}^{v}, \mathbf{e}^{w}\right\}$ para a base dual. Para um vetor $\boldsymbol{\lambda}$, denotamos suas componentes em relação a $\left\{\mathbf{e}_{i}\right\}$ como $\lambda^i$ e suas componentes com relação a $\left\{\mathbf{e}^{i}\right\}$ como $\lambda_i$.
Assim, podemos reescrever as equações \eqref{eq:LambdaEmAmbasAsBases} como 
\[
\begin{array}{l}{\boldsymbol{\lambda}=\lambda^{1} \mathbf{e}_{1}+\lambda^{2} \mathbf{e}_{2}+\lambda^{3} \mathbf{e}_{3}=\displaystyle\sum_{i=1}^{3} \lambda^{i} \mathbf{e}_{i}} \\ {\boldsymbol{\lambda}=\lambda_{1} \mathbf{e}^{1}+\lambda_{2} \mathbf{e}^{2}+\lambda_{3} \mathbf{e}^{3}=\displaystyle\sum_{i=1}^{3} \lambda_{i} \mathbf{e}^{i}}\end{array} .
\]

Além disso, podemos economizar mais nossa notação utilizando a convenção de somatório. Isto é, concordarmos que sempre que um sufixo aparece duas vezes, \textit{uma como subscrito e uma como sobrescrito}, então o somatório ao longo da extensão indicada pelo sufixo é implicada, \textit{sem o uso de $\sum_{i=1}^3$ para indicar o somatório}(analogamente para os outros tipos de sufixos). Assim, podemos reduzir ainda mais as equações para
\begin{equation}\label{eq:LambdaEmAmbasAsBasesConvencaoSomatorio}
	\boldsymbol{\lambda}=\lambda^{i} \mathbf{e}_{i} \qq{e} \boldsymbol{\lambda}=\lambda_{i} \mathbf{e}^{i} .
\end{equation}

As componentes $\lambda^i$ de um vetor $\boldsymbol{\lambda}$ que provêm do uso da base natural são chamados de \textit{componentes contravariantes}, enquanto os componentes $\lambda_i$ que surgem do uso da base dual são chamados de \textit{componentes contravariantes}.

Utilizando as definições de $\mathbf{e}^i$ e $\mathbf{e}_j$, temos
\[
	\mathbf{e}^{i} \cdot \mathbf{e}_{j}=\nabla u^{i} \cdot \frac{\partial \mathbf{r}}{\partial u^{j}}=\frac{\partial u^{i}}{\partial x} \frac{\partial x}{\partial u^{j}}+\frac{\partial u^{i}}{\partial y} \frac{\partial y}{\partial u^{j}}+\frac{\partial u^{i}}{\partial z} \frac{\partial z}{\partial u^{j}}=\frac{\partial u^{i}}{\partial u^{j}} ,
\]
onde utilizamos a regra da cadeia da derivação parcial para simplificar os termos. Se $i=j$, temos que $\partial u^{i} / \partial u^{j}=1$, e caso contrário, $\partial u^{i} / \partial u^{j}=0$. Assim, podemos escrever
\begin{equation}\label{eq:ProdutoInternoBase}
	\mathbf{e}^{i} \cdot \mathbf{e}_{j}=\delta_{j}^{i} ,
\end{equation}
onde $\delta^i_j$ é o \textit{delta de Kronecker}, definido por 
\begin{equation}\label{eq:DeltaKroneckerDefinicao}
\delta_{j}^{i}=\left\{\begin{array}{l}{1, \text { para } i=j} \\ {0, \text { para } i \neq j}\end{array}\right. .
\end{equation}
Assim, podemos dizer que as componentes contravariantes podem ser obtidas por meio do produto interno do vetor com a base dual:
\[
	\boldsymbol{\lambda} \cdot \mathbf{e}^{j}=\lambda^{i} \mathbf{e}_{i} \cdot \mathbf{e}^{j}=\lambda^{i} \delta_{i}^{j}=\lambda^{j}
\]
e, analogamente, as componentes covariantes a partir da base natural:
\[\lambda_{j}=\boldsymbol{\lambda} \cdot \mathbf{e}_{j}.\]
A simplificação $\lambda^i\delta^j_i=\lambda^j$ é justificada porque o único termo não nulo na soma ocorre quando $i=j$ e o delta é igual a 1 nesse caso. Assim, ele tem o efeito de substituir $j$ por $i$ e ``retirar'' o somatório.
Usando as componentes contravariantes, podemos escrever o produto interno entre dois vetores $\boldsymbol{\lambda}$ e $\boldsymbol{\mu}$ como 
\begin{equation*}
	\boldsymbol{\lambda} \cdot \boldsymbol{\mu}=\lambda^{i} \mathbf{e}_{i} \cdot \mu^{j} \mathbf{e}_{j}=g_{i j} \lambda^{i} \mu^{j} ,
\end{equation*}
onde
\begin{equation}\label{eq:DefinicaoTensorMetricaContravariante}
	\boxed{
		g_{i j} \equiv \mathbf{e}_{i} \cdot \mathbf{e}_{j}  .
	}
\end{equation}
Analogamente, utilizando componentes covariantes, podemos escrevê-lo como
\begin{equation*}
	\boldsymbol{\lambda} \cdot \boldsymbol{\mu}=\lambda_{i} \mathbf{e}^{i} \cdot \mu_{j} \mathbf{e}^{j}=g^{i j} \lambda_{i} \mu_{j}, 
\end{equation*}
onde
\begin{equation}\label{eq:DefinicaoTensorMetricaCovariante}
	\boxed{
		g^{i j} \equiv \mathbf{e}^{i} \cdot \mathbf{e}^{j}  .
	}
\end{equation}
Também podemos misturar e escrever como
\[
	\boldsymbol{\lambda} \cdot \boldsymbol{\mu}=\lambda_{i} \mathbf{e}^{i} \cdot \boldsymbol{\mu}^{j} \mathbf{e}_{j}=\lambda_{i} \mu^{j} \delta_{j}^{i}=\lambda_{i} \mu^{i}
\]
Assim, temos quatro modos diferentes de escrever o produto interno:
\begin{equation}\label{eq:ProdutoInterno}
	\boxed{
		\boldsymbol{\lambda} \cdot \boldsymbol{\mu}=g_{ij}\lambda^i\mu^j = g^{ij}\lambda_i\mu_j = \lambda_i\mu^i = \lambda^i\mu_i .
	}
\end{equation}
O fato que $g^{ij}\lambda_i\mu_j=\lambda_i\mu^i$ vale para componentes arbitrárias $\lambda_i$ implica que 
\begin{equation}\label{eq:LevantamentoIndice}
	g^{ij}\mu_j=\mu^i ,
\end{equation}
mostrando que as quantidades $g^{ij}$ pode ser utilizada para levantar o sufixo para obter as componentes contravariantes de $\mu$ a partir das covariantes. Analogamente, temos 
\begin{equation}\label{eq:AbaixamentoIndice}
	g_{ij}\mu^j=\mu_i ,
\end{equation}
mostrando que as quantidades $g_{ij}$ revertem a operação, abaixando-se os sufixos. Combinando as operações, temos
\[
	\mu^i = g^{ij}\mu_j=g^{ij}g_{ik}\mu^k ,
\]
e, porque isso vale para componentes $\mu^i$ arbitrárias, podemos deduzir que
\begin{equation}
	\boxed{
		g^{ij}g_{jk}=\delta^i_k   .
	}
\end{equation}

A partir das definições dos produtos internos dos vetores da base, pode-se perceber que essas quantidades são simétricas, isto é,
\begin{equation}\label{eq:SimetriaTensorMetrica}
	g_{ij}=g_{ji}, \qquad g^{ij}=g^{ji} .
\end{equation}

\section{Tangentes e gradientes}\label{sec:TangentesGradientes}

Vamos olhar para os vetores tangentes a uma curva no espaço.
Suponha que temos as relações
\begin{equation}\label{eq:CoordenadasFuncoesdeT}
	u=u(t), \quad v=v(t), \quad w=w(t) ,
\end{equation}
onde $u(t),v(t),w(t)$ são funções diferenciaveis de $t$, onde este pertence a algum intervalo $I$. Então os pontos cujas coordenadas são dadas por \eqref{eq:CoordenadasFuncoesdeT} são parte de uma curva $\gamma$ parametrizada por $t$. O vetor posição desses pontos é 
\[
	\mathbf{r}(t)=x(u(t), v(t), w(t)) \mathbf{i}+y(u(t), v(t), w(t)) \mathbf{j}+z(u(t), v(t), w(t)) \mathbf{k}	
\]
e, para cada $t$ em $I$, a derivada $\dot{\mathbf{r}} \equiv d\mathbf{r}/dt$ nos dá um vetor tangente à curva (desde que não seja nula). Utilizando a regra da cadeia, temos
\[
	\frac{d \mathbf{r}}{d t}=\frac{\partial \mathbf{r}}{\partial u} \frac{d u}{d t}+\frac{\partial \mathbf{r}}{\partial v} \frac{d v}{d t}+\frac{\partial \mathbf{r}}{\partial w} \frac{d w}{d t} ,
\]
que pode ser escrito como

\begin{equation}\label{eq:DerivadaPosicaoTangente}
	\dot{\mathbf{r}}(t)=\dot{u}(t)\mathbf{e}_u+\dot{v}(t)\mathbf{e}_v+\dot{w}(t)\mathbf{e}_w = \dot{u}^i(t)\mathbf{e}_i.
\end{equation}

O comprimento da curva $\gamma$ é obtido integrando $|\dot{\mathbf{r}}|$ com respeito a $t$ ao longo do intervalo $I$. Mas
\[
	|\dot{\mathbf{r}}|^{2}=\dot{\mathbf{r}} \cdot \dot{\mathbf{r}}=\dot{u}^{i} \mathbf{e}_{i} \cdot \dot{u}^{j} \mathbf{e}_{j}=g_{i j} \dot{u}^{i} \dot{u}^{j},
\]
utilizando a equação \eqref{eq:DefinicaoTensorMetricaContravariante}. Assim, se $I$ é dado por $a\leq t\leq b$, então o comprimento de $\gamma$ é dado por
\begin{equation}\label{eq:ComprimentoCurva}
	\boxed{
	L=\int_{a}^{b}\left(g_{i j} \dot{u}^{i} \dot{u}^{j}\right)^{1 / 2} dt .
	}
\end{equation}
A versão infinitesimal da equação \eqref{eq:DerivadaPosicaoTangente} é $d\mathbf{r}=du^i\mathbf{e}_i$, o que nos dá
\[
	ds^2=d\mathbf{r}\cdot d\mathbf{r}=du^i\mathbf{e}_i \cdot du^j\mathbf{e}_j 
\]
para pontos cujas coordenadas diferem por $du^i$. Assim, chegamos na fórmula 
\begin{equation}
	\boxed{
		ds^2=g_{ij}du^i du^j,
	}
\end{equation}
que é a generalização da fórmula cartesiana $ds^2=dx^2+dy^2+dz^2$.

Suponha, agora, uma função diferenciável $\phi(u,v,w,)$. Isso nos dará uma função da posição e, portanto, um campo escalar. Seu gradiente é
\[
	\nabla \phi \equiv \frac{\partial \phi}{\partial x} \mathbf{i}+\frac{\partial \phi}{\partial y} \mathbf{j}+\frac{\partial \phi}{\partial z} \mathbf{k} ,
\]
onde, ao calcularmos essas derivadas parciais, tratamos $\phi$ como uma função de $x,y,z$ ao substituirmos as expressões de $u,v,w$ em termos de $x,y,z$ dadas pela equação \eqref{eq:TransformacaoUX}:
\[
	\phi = \phi(u(x,y,z),\,v(x,y,z),\,w(x,y,z)) .
\]
A regra da cadeia nos dá 
\[
	\frac{\partial \phi}{\partial x}=\frac{\partial \phi}{\partial u} \frac{\partial u}{\partial x}+\frac{\partial \phi}{\partial v} \frac{\partial v}{\partial x}+\frac{\partial \phi}{\partial w} \frac{\partial w}{\partial x}	
,\]
com expressões análogas para $\partial\phi/\partial y$ e $\partial\phi/\partial z$. Assim, podemos dizer que
\begin{align*} \nabla \phi=\frac{\partial \phi}{\partial u}\left(\frac{\partial u}{\partial x}\right.&\left.\mathbf{i}+\frac{\partial u}{\partial y} \mathbf{j}+\frac{\partial u}{\partial z} \mathbf{k}\right)+\frac{\partial \phi}{\partial v}\left(\frac{\partial v}{\partial x} \mathbf{i}+\frac{\partial v}{\partial y} \mathbf{j}+\frac{\partial v}{\partial z} \mathbf{k}\right) \\ &+\frac{\partial \phi}{\partial w}\left(\frac{\partial w}{\partial x} \mathbf{i}+\frac{\partial w}{\partial y} \mathbf{j}+\frac{\partial w}{\partial z} \mathbf{k}\right) \\=& \frac{\partial \phi}{\partial u} \nabla u+\frac{\partial \phi}{\partial v} \nabla v+\frac{\partial \phi}{\partial w} \nabla w .
\end{align*}
Isto é,
\begin{equation}\label{eq:GradientePhi}
	\nabla\phi = \pdv{\phi}{u^i}\mathbf{e}^i,
\end{equation}
mostrando que as derivadas parciais $\partial\phi/\partial u^i$ são as componentes de $\nabla\phi$ para a base dual $\{\mathbf{e}^i\}$. Note que, considerar o sufixo repetido como uma indicação de somatório na equação \eqref{eq:GradientePhi}, estamos tratando o sufixo $i$ em $\partial\phi/\partial u^i$ como um subscrito. Podemos deixar isso mais claro ao denotarmos o operador de diferenciação parcial $\partial/\partial u^i$ para $\partial_i$, de modo que $\partial\phi/\partial u^i=\partial_i\phi$. A notação $\phi_{,i}$ também é utilizada para os mesmos fins. Assim, podemos reescrever a equação \eqref{eq:GradientePhi} como 
\begin{equation}\label{eq:GradientePhiNotacao}
	\nabla\phi=\partial_\phi \mathbf{e}^i = \phi_{,i}\mathbf{e}^i,
\end{equation}
com o sufixo sendo utilizado, corretamente, como subscrito. 

Assim, vemos que, ao lidarmos com tangentes a curvas, é apropriado utilizar a base natural, mas ao tratarmos campos escalares, é mais fácil utilizar a base dual.


\section{Transformações de coordenadas em espaços euclidianos}\label{sec:TransformacoesCoordenadas}

Suponha que temos dois sistemas de coordenadas curvilíneos no espaço euclidiano, denotados por $(u,v,w)$ e $(u^\prime,v^\prime,w^\prime)$. Podemos distingui-los ao chamá-los de coordenadas 'sem linha' e 'com linha', respectivamente. Vamos estudar como as componentes de vetores relativas às bases definidas pelos sistemas coordenados são transformadas quando passamos de um referencial sem linha para um linha (ou \textit{vice versa}). As coordenadas do primeiro serão denotadas por $u^i$, ao passo que no sistema linha, as coordenadas serão $u^{i^\prime}$. Seguindo a notação, temos que a base natural é dada por $\{\mathbf{e}_{i^\prime}\}$, enquanto a base dual é $\{\mathbf{e}^{i^\prime}\}$. Assim, podemos escrever
\begin{equation}\label{eq:CoordenadasLinha}
	\boldsymbol{\lambda}=\lambda^{i^\prime}\mathbf{e}_{i^\prime} , 
\end{equation}
com uma expressão similar para a base dual.

Na região do espaço coberta por ambos sistemas de coordenadas, temos equações $u^{i^{\prime}}=u^{i^{\prime}}\left(u^{j}\right)$ e inversas $u^{i}=u^{i}\left(u^{j^\prime}\right)$. Por definição, temos $\mathbf{e}_i \equiv \pdv*{\mathbf{r}}{u^i}$ e $\mathbf{e}_{i^\prime} \equiv \pdv*{\mathbf{r}}{u^{i^\prime}}$. Utilizando a regra da cadeia,
\[
	\frac{\partial \mathbf{r}}{\partial u^{j}}=\frac{\partial \mathbf{r}}{\partial u^{i^{\prime}}} \frac{\partial u^{i^{\prime}}}{\partial u^{j}}.
\]
Podemos, então, escrever
\begin{equation}\label{eq:TransformadaContravarianteBase}
	\mathbf{e}_j = U^{i^\prime}_j \mathbf{e}_{i^\prime}, 
\end{equation}
onde $U^{i^\prime}_j $ é um símbolo que representa a derivada parcial $\pdv*{u^{i^\prime}}{u^j}$. Temos, então
\[
	\boldsymbol{\lambda}=\lambda^{j} \mathbf{e}_{j}=\lambda^{j} U_{j}^{i^{\prime}} \mathbf{e}_{i^{\prime}}.
\]
Comparando com a equação \eqref{eq:CoordenadasLinha}, chegamos em
\begin{equation}\label{eq:TransformadaCoordenadasContravariante}
	\boxed{
		\lambda^{i^\prime} = U^{i^\prime}_j \lambda^j
	}
\end{equation}
como a fórmula de transformação para as coordenadas contravariantes de um vetor. Similarmente para as coordenadas covariantes.

Por definição, $\mathbf{e}^{i} \equiv \nabla u^{i} \text { e } \mathbf{e}^{i^{\prime}} \equiv \nabla u^{i^{\prime}}$. A regra da cadeia nos dá
\[
	\frac{\partial u^{j}}{\partial x}=\frac{\partial u^{j}}{\partial u^{i^{\prime}}} \frac{\partial u^{i^{\prime}}}{\partial x},
\]
com expressões similares para $\pdv*{u^j}{y}$ e $\pdv*{u^j}{z}$. Então
\[
	\nabla u^{j}=\frac{\partial u^{j}}{\partial u^{i^{\prime}}} \nabla u^{i^{\prime}},
\]
de modo que podemos escrever como
\begin{equation}\label{eq:TransformadaCovarianteBase}
	\mathbf{e}^{j} = U^{j}_{i^\prime} \mathbf{e}^{i^\prime}.
\end{equation}
Assim, para componentes covariantes, temos
\[
	\boldsymbol{\mu}=\mu_{j} \mathbf{e}^{j}=\mu_{j} U_{i^{\prime}}^{j} \mathbf{e}^{i^{\prime}}
\]
e, ao compararmos com $\mathbf{\mu}=\mu_{i^\prime}\mathbf{e}^{i^\prime}$ nos dá
\begin{equation}\label{eq:TransformadaCoordenadasCovariante}
	\boxed{
		\mu_{i^\prime} = U^{j}_{i^\prime} \mu_j
	}
\end{equation}
como a fórmula de transformação para componentes covariantes.

Como as coordenadas nenhuma das coordenadas 'linha' ou 'sem linha' é privilegiada, isto é, elas podem trocar de papel, as transformadas inversas são dadas apenas trocando os sufixos. Isso nos dá
\begin{equation}\label{eq:TransformadasBaseInversa}
	\mathbf{e}_{j^{\prime}}=U_{j^{\prime}}^{i} \mathbf{e}_{i}, \quad \mathbf{e}^{j^{\prime}}=U_{i}^{j^{\prime}} \mathbf{e}^{i} 
\end{equation}
para as transformações de base e
\begin{equation}\label{eq:TransformadasCoordenadasInversas}
	\lambda^{i}=U_{j^{\prime}}^{i} \lambda^{j^{\prime}}, \quad \mu_{i}=U_{i}^{j^{\prime}} \mu_{j^{\prime}}
\end{equation}
para as componentes. Combinando esta equação com a equação \eqref{eq:TransformadaCoordenadasContravariante}, trocando o índice $j$ por $k$ nesta última, chegamos na relação
\begin{equation}
	\boxed{
		U_{i}^{k} U_{j}^{i^{\prime}}=\delta_{j}^{k} .
	}
\end{equation}

No sistema de coordenda com linha, as quantidades correspondentes a $g_{ij}$ são definidas por $g_{i^{\prime} j^{\prime}} \equiv e_{i^{\prime}} \cdot e_{j^{\prime}}$. Usando as transformadas das bases, temos
\[
	g_{i^{\prime} j^{\prime}}=\mathbf{e}_{i^{\prime}} \cdot \mathbf{e}_{j^{\prime}}=\left(U_{i^{\prime}}^{k} \mathbf{e}_{k}\right) \cdot\left(U_{j^{\prime}}^{l} \mathbf{e}_{l}\right)=U_{i^{\prime}}^{k} U_{j^{\prime}}^{l} \mathbf{e}_{k} \cdot \mathbf{e}_{l}=U_{i^{\prime}}^{k} U_{j^{\prime}}^{l} g_{k l} .
\]
Assim, a fórmula de transformação para as quantidades $g_{ij}$ é
\begin{equation}\label{TransformadaTensorMetricaContravariante}
	\boxed{
		g_{i^{\prime} j^{\prime}}=U_{i^{\prime}}^{k} U_{j^{\prime}}^{l} g_{k l} .
	}
\end{equation}
Analogamente, temos
\begin{equation}\label{TransformadaTensorMetricaCovariante}
	\boxed{
		g^{i^{\prime} j^{\prime}}=U^{i^{\prime}}_{k} U^{j^{\prime}}_{l} g^{k l} 
	}
\end{equation}
para as quantidades $g^{ij}$.

As equações \eqref{TransformadaTensorMetricaContravariante} e \eqref{eq:TransformadaCoordenadasCovariante} nos dão uma primeira amostragem de como as componentes de um tensor se transformam. As quantidades $g_{ij}$ são as componentes do \textit{tensor de métrica}, chamado assim porque ele nos mostra as propriedades métricas tais como tamanhos de vetores e os ângulos entre eles (por meio do produto interno $\boldsymbol{\lambda}\cdot\boldsymbol{\mu}=g_{ij}\lambda^i\mu^j$) e a distância entre dois pontos vizinhos (por meio do elemento de linha $ds^2=g_{ij}du^idu^j$).





% \section{*Campos tensoriais no espaço euclidiano}\label{sec:CamposTensoriais}

% Embora campos escalares e vetoriais sejam o suficiente para formular a Teoria da Gravitação Universal de Newton, a teoria de Einstein requer o uso de campos tensoriais. 


\section{Variedades}\label{sec:Variedades}

O modelo para o espaço-tempo na Relatividade Geral utiliza uma variedade quadridimensional. Dessa forma, precisamos estudar o que isso envolve.

O que torna uma variedade $N$-dimensional é que seus pontos podem ser rotulados por um sistema de $N$ coordenadas reais $x^1,x^2,\ldots,x^N$, de modo que haja uma correspondência de um para um entre os pontos e as coordenadas. Além disso, vamos trabalhar com variedades dotadas de um campo tensorial de métrica, que não é um requerimento para variedades no geral. Não é necessário que toda uma variedade $M$ seja coberta por um único sistema de coordenadas e não há sistema preferencial. O caso geral é que há uma coleção de sistemas coordenados, cada um cobrindo uma região de $M$. Quando dois sistemas se sobrepõem, há um conjunto de equações dando cada coordenada de um sistema em função das coordenadas do outro. Assim, se as coordenadas $x^a$ cobrem uma região $U$ e as coordenadas $x^{a^\prime}$ cobrem a região $U^\prime$ (com essas regiões se interceptando), então as coordendas dos pontos na região de sobreposição são relacionadas por equações da forma
\begin{equation}\label{eq:TransformacaoVariaveisLinha}
	x^{a^\prime}=x^{a^\prime}\left(x^{1}, x^{2}, \ldots, x^{N}\right) \quad(a=1, \ldots, N) ,
\end{equation}
dando cada $x^{a^\prime}$ como uma função das coordenadas $x^b$, e essas tendo inversas da forma
\begin{equation}\label{eq:TransformacaoVariaveisLinhaInversas}
	x^{a}=x^{a}\left(x^{1^\prime}, x^{2^\prime}, \ldots, x^{N^{\prime}}\right) \quad(a=1, \ldots, N),
\end{equation}
dando cada $x^a$ como uma função das coordenadas $x^{b^\prime}$. Vamos assumir que as funções envolvidas são diferenciaveis, de modo que as derivadas parciais
\[
	X_{b}^{a^{\prime}} \equiv \frac{\partial x^{a^{\prime}}}{\partial x^{b}} \quad \text { e } \quad X_{b^{\prime}}^{a} \equiv \frac{\partial x^{a}}{\partial x^{b^{\prime}}}
\]
existam. A matrix $N\times N$ $[X^{a^\prime}_b]$ é a \textit{matriz jacobiana} associada às equações \eqref{eq:TransformacaoVariaveisLinha}. O fato que essas equações possuem inversas \eqref{eq:TransformacaoVariaveisLinhaInversas} significa que essa matriz é inversível, de modo que o Jacobiano $\det[X^{a^\prime}_b]$ é não nulo em cada ponto da região de intersecção. O mesmo pode ser dito para a matriz jacobiana $[X^a_{b^\prime}]$ e seu jacobiano $\det[X^a_{b^\prime}]$. De fato, ambas as matrizes são inversas, uma vez que a regra da cadeia nos dá
\begin{equation}\label{eq:JacobianaInversa}
	X^a_{b^\prime}X^{b^\prime}_c=\delta^a_c.
\end{equation}
Trocando o papel das coordenadas com linha e sem linha, chegamos na relação
\begin{equation}\label{eq:JacobianaInversaLinha}
	X^{a^\prime}_bX^b_{c^\prime}=\delta^a_c .
\end{equation}

Vamos definir vetores e campos vetoriais em uma variedade como objetos possuindo $N$ componentes de modo que, em uma mudança de variáveis, eles generalizam a equação \eqref{eq:TransformadaCoordenadasContravariante} ou \eqref{eq:TransformadaCoordenadasCovariante} para componentes de vetores no espaço euclidiano. Assim, definimos um \textit{vetor contravariante} em um ponto $P$ como sendo um objeto com $N$ componentes $\lambda^a$ as quais, sob uma mudança de variáveis em $P$ transformam-se de acordo com
\begin{equation}\label{eq:TransformadaCoordenadasContraVariedade}
	\boxed{
		\lambda^{a^\prime} = X^{a^\prime}_b\lambda^b ,
	}
\end{equation}
em que as derivadas parciais são tomadas em $P$. Um \textit{vetor covariante} é definido similarmente, cujas componentes $\mu_a$ transformam-se de acordo com
\begin{equation}
	\boxed{
		\mu_{a^\prime}=X^b_{a^\prime}\mu_b .
	}
\end{equation}
 Para campos vetoriais, essas leis de transformação são válidas em cada ponto em que o campo é definido.

 Um exemplo de vetor contravariante é o vetor tangente a uma curva $\gamma$, que pode ser dado parametricamente por $x^a=x^a(t)$, onde $x^a(t)$ são funções diferenciaveis de $t$ para $t$ em algum intervalo $I$. 

Agora, suponha que para cada sistema de coordenadas em um ponto $P$ de uma variedade $M$ há $N^{r+s}$ quantidades $\tau^{a_1\ldots a_r}_{b_1\ldots b_s}$ que, sob uma mudança de coordenadas, transformam-se de acordo com
\begin{equation}\label{eq:TransformacaoCoordenadaTensor}
	\boxed{
		\tau_{b_{1}^{\prime} \ldots b_{s}^{\prime}}^{a_{1}^{\prime} \ldots a_{r}^{\prime}}=X_{c_{1}}^{a_{1}^{\prime}} \cdots X_{c_{r}}^{a_{r}^{\prime}} X_{b_{1}^{\prime}}^{d_{1}} \cdots X_{b_{s}^{\prime}}^{d_{s}} \tau_{d_{1} \ldots d_{s}}^{c_{1} \ldots c_{r}}   ,
	}
\end{equation}
onde as matrizes jacobianas $[X^{a^\prime}_c],[X^d_{b^\prime}]$ são calculadas em $P$. Então, essas quantidades são as \textit{componentes de um tensor do tipo $(r,s)$ em $P$}. O mesmo vale para o caso especial em que $r=0$ ou $s=0$, de modo que a letra base $\tau$ apenas possua subscritos ou sobrescritos. Por exemplo, as componentes de um tensor do tipo $(2,0)$ transformam-se de acordo com $\tau^{a^{\prime} b^{\prime}}=X_{c}^{a^{\prime}} X_{d}^{b^{\prime}} \tau^{c d}$.

A soma $(r+s)$ é chamada de ordem do tensor. Um tensor do tipo $(r,0)$ é chamado de \textit{tensor contravariante} de ordem $r$, ao passo que um tensor do tipo $(0,s)$ é chamado de $tensor covariante$ de ordem $s$. Se $rs\neq0$, o tensor é descrito como \textit{misto}. Agora podemos reconhecer um vetor contravariante em $P$ como um tensor do tipo $(1,0)$, um vetor covariante como um tensor do tipo $(0,1)$ e um escalar como um  do tipo $(0,0)$.

Se em cada ponto de uma região $N$-dimensional $V$ em $M$ existir um tensor do tipo $(r,s)$, então nós temos um \textit{campo tensorial} em $V$. Podemos, também, ter tensores definidos em cada ponto de uma curva $\gamma$ de $M$ mas não uma região $N$-dimensional (tais como os vetores tangentes a uma curva). Estes constituem um \textit{campo tensorial ao longo de $\gamma$}. O mesmo se aplica a um campo tensorial definido em uma superfície $\Sigma$ em $M$.

Vamos considerar, agora, algumas operações com tensores. A primeira é a \textit{adição}: somar componentes correspondentes de dois tensores do mesmo tipo resulta em quantidades que são componentes de um tensor do mesmo tipo. A segunda é a \textit{multiplicação por escalares}: multiplicar cada componente de um tensor por um escalar resulta em quantidades que são componentes de um tensor do mesmo tipo. A validade dessas operações segue diretamente da fórmula de transformação de componentes.

A terceira operação é a \textit{multiplicação de tensores}, que dá o produto tensorial entre dois destes. As componentes de um produto tensorial são obtidas ao multiplicarem-se as componentes de ambos os tensores envolvidos, de modo a formar todos os produtos possíveis. Por exemplo, se colocarmos $\sigma^a_{bc}\equiv\lambda_b\tau^a_c$, onde $\lambda_b$ são as componentes de um tensor do tipo $(0,1)$ que se transformam de acordo com $\lambda_{b^\prime}=X^e_{b^\prime}\lambda_e$ e $\tau^a_c$ são as de um tensor do tipo $(1,1)$ que se transformam sob $\tau^{a^\prime}_{c^\prime}=X^{a^\prime}_dX^f_{c^\prime}\tau^d_f$, então para $\sigma^{a^\prime}_{b^\prime c^\prime}$, nós temos
\[
\begin{aligned} \sigma_{b^{\prime} c^{\prime}}^{a^{\prime}} &=\left(X_{b^{\prime}}^{e} \lambda_{e}\right)\left(X_{d}^{a^{\prime}} X_{c^{\prime}}^{f} \tau_{f}^{d}\right) \\ &=X_{d}^{a^{\prime}} X_{b^{\prime}}^{e} X_{c^{\prime}}^{f} \sigma_{e f}^{d}  ,
\end{aligned}
\]
mostrando que $\sigma^a_{bc}$ são as componentes de um tensor do tipo $(1,2)$.

A quarta operação é a de \textit{contração}. Ela pode ser aplicada a qualquer conjunto de números simbolizados por letras com subscritos e sobrescritos. Se há $r$ sobrescritos e $s$ subscritos, então há $rs$ modos em que isso pode ser feito, cada um gerando uma contração do conjunto de números original. Se essa operação for feita com um tensor do tipo $(r,s)$, então suas contrações são componentes de um tensor do tipo $(r-1,s-1)$. A prova para o caso geral é trabalhosa e por isso vamos dar apenas um exemplo que ilustra bem esse resultado.


\noindent
\rule{\textwidth}{0.4pt}
\begin{exemplo}
Sejam $\tau^{ab}_c$ as componentes de um tensor tipo $(2,1)$, e $\sigma^a=\tau^{ab}_b$ uma contração. Usando coordenadas linha, nós formaríamos, analogamente, $\sigma^{a^\prime}=\tau^{a^\prime b^\prime}_{b^\prime}$. Assim,
\[
	\sigma^{a^{\prime}}=\tau_{b^{\prime}}^{a^{\prime} b^{\prime}}=\tau_{e}^{c d} X_{c}^{a^{\prime}} X_{d}^{b^{\prime}} X_{b^{\prime}}^{e}=\tau_{e}^{c d} X_{c}^{a^{\prime}} \delta_{d}^{e}=\tau_{d}^{c d} X_{c}^{a^{\prime}}=\sigma^{c} X_{c}^{a^{\prime}} ,
\]
mostrando que os números $\sigma^a$ são as componentes de um tensor do tipo $(1,0)$.

\noindent
\rule{\textwidth}{0.4pt}	
\end{exemplo}

Um tensor especial é o \textit{tensor de Kronecker}, que é um tensor do tipo $(1,1)$ com a propriedade de que em qualquer sistema de coordenadas usado, suas componentes $\kappa^a_b$ são dadas pelo delta de Kronecker $\delta^a_b$. Para ver isso, suponha que, em um sistema de coordenadas sem linha, nós tenhamos $\kappa_b^a=\delta^a_b$. Então, quando transformamos para um sistema de coordenado com linha, as componentes se tornam
\[
	\kappa^{a^\prime}_{b^\prime} = X^{a^\prime}_c X^d_{b^\prime}\delta^c_d = X^{a^\prime}_c X^c_{b^\prime} = \delta^a_b .
\]
Por causa disso, denotaremos suas componentes por $\delta^a_b$ em vez de $\kappa^a_b$ (O que foi mostrado aqui é que a propriedade $\kappa^a_b=\delta^a_b$ é independente de coordendas).

Vamos agora analisar como podemos associar tensores de diferentes tipos ao contrai-los com os tensores de métrica covariante ou contravariante.

Como dito no início da seção, assumimos que esta variedade possui um tensor de métrica em cada ponto $g_{ab}$. Esse tensor é simétrico, de modo que $g_{ab}=g_{ba}$, e é não singular, de modo que a matriz $[g_{ab}]$ possui inversa $[g^{ab}]$ cujos elementos satisfazem
\begin{equation}\label{eq:TensorMetricaDelta}
	g^{ab}g_{bc}=\delta^a_c .
\end{equation}
Como $[g_{ab}]$ é uma matriz simétrica, sua inversa também o é, então $g^{ab}=g^{ba}$.

Nós temos agora um tensor de métrica com componentes $g_{ab}$ que podem ser usadas para abaixar sufixos e o tensor de métrica contravariante com componentes $g^{ab}$ que podem ser usadas para levantar sufixos. Por exemplo, se $\tau^{ab}_c$ são as componentes de um tensor do tipo $(2,1)$, então usando o tensor de métrica para abaixar o primeiro sobrescrito nós dá um tensor do tipo $(1,2)$ cujas componentes são definidas por $\tensor{\tau}{_a^b_c} = g_{ad}\tau^{db}_c$. Se o tensor de métrica contravariante é utilizado para levantar o sufixo abaixado, o tensor original é recuperado: $g^{a d} \tau_{d c}^{b}=g^{a d} g_{d e} \tau_{c}^{e b}=\delta_{e}^{a} \tau_{c}^{e b}=\tau_{c}^{a b}$.

Tensores que podem ser obtidos ao se levantarem ou abaixarem sufixos uns dos outros são chamados de \textit{associados}, e iremos utilizar a mesma letra base para as componentes, como no exemplo anterior. Isso pode ser ambíguo pelo fato de que mais de um tensor do mesmo tipo pode ser associado a um dado tensor. Por exemplo, abaixando-se o primeiro sobrescrito das componentes $\tau^a_b$ de um tensor tipo $(2,0)$ nos dá um de tipo $(1,1)$ que é, em geral, diferente do obtido ao abaixar-se o segundo sobrescrito. A distinção pode ser feita ao utilizar-se um espaçamento nos sufixos:
\[
	\tensor{\tau}{_a^b} = g_{ac}\tau^{cb}, \qquad \tensor{\tau}{^a_b}=g_{bc}\tau^{ac} .
\]
Os tensores de métrica covariante, contravariante e o tensor de Kronecker são associados, visto que $\delta_{b}^{a}=g^{a c} g_{c b}$ e $g^{a b}=g^{a d} \delta_{d}^{b}=g^{a d} g^{b c} g_{c d}$. No entanto, utilizamos $\delta^a_b$ em vez de $g^a_b$ pela forma especial de suas componentes.

A partir de agora, utilizaremos uma convenção de ``confundir'' um tensor com suas componentes. Isto é, vamos utilizar a nomenclatura de \textit{tensor} $\tau^{ab}$ em vez de \textit{tensor com componentes} $\tau^{ab}$.

\section{Propriedades da métrica}\label{sec:Metrica}

O tensor de métrica $g_{ab}$ nos dá um produto interno $g_{ab}\lambda^a\mu^b$ para vetores $\lambda^a,\mu^a$ em cada ponto $P$ de uma variedade $M$. Assim como no espaço euclidiano \eqref{eq:ProdutoInterno}, há quantro modos de escrevê-lo:
\begin{equation}
	\boxed{
	g_{ab}\lambda^a\mu^b= g^{ab}\lambda_a\mu_b = \lambda_a\mu^a = \lambda^a\mu_a .
	}
\end{equation}
Usualmente, pede-se que um produto interno seja \textit{positivo definido}, isto é, $g_{ab}\lambda^a\lambda^b\geq 0$ para todos os vetores $\lambda^a$ e $g_{ab}\lambda^a\lambda^b=0 \Leftrightarrow \lambda^a=0$. Uma variedade com tensor de métrica positivo definido é chamada de \textit{riemanniana}. No entanto, para podermos modelar o espaço-tempo na Relatividade Geral, vamos ``relaxar'' essa condição e pedir apenas que o tensor de métrica seja não-singular, de modo que a matriz $[g_{ab}]$ seja inversível. Assim, o espaço-tempo de RG é uma variedade chamada \textit{pseudo-riemanniana}. Vamos agora fazer algumas definições de modo a generalizar as propriedades para o espaço euclidiano.

O \textit{módulo} de um vetor $\lambda^a$ é dado por
\begin{equation}\label{eq:ModuloVetorVariedade}
	\left|g_{a b} \lambda^{a} \lambda^{b}\right|^{1 / 2}=\left|g^{a b} \lambda_{a} \lambda_{b}\right|^{1 / 2}=\left|\lambda_{a} \lambda^{a}\right|^{1 / 2}
\end{equation}
Um vetor \textit{unitário} é um cujo módulo é 1. Se estamos em uma variedade pseudo-riemanniana, podemos ter que $|\lambda_a\lambda^a|^{1/2}=0$ para $\lambda^a\neq0$. Neste caso, dizemos que o vetor $\lambda^a$ é \textit{nulo}.

O \textit{ângulo} $\theta$ entre dois vetores $\lambda^a,\mu^a$ não nulos é dado por
\[
	\cos \theta=\frac{g_{a b} \lambda^{a} \mu^{b}}{\left|g_{c d} \lambda^{c} \lambda^{d}\right|^{1 / 2}\left|g_{e f} \mu^{e} \mu^{f}\right|^{1 / 2}}
\]
Se o tensor de métrica é indefinido, essa fórmula pode nos dar $|\cos\theta|>1$, resultando em um valor não-real para $\theta$. Dois vetores são \textit{ortogonais} ou \textit{perpendiculares} se seu produto interno for zero.

Como explicado na seção \ref{sec:Variedades}, uma curva $\gamma$ em uma variedade é dada ao colocar $x^a=x^a(t)$, onde o parâmetro $t$ pertence a algum intervalo $I$, e em cada ponto de $\gamma$ um vetor tangente é dado por $\dot{x}^a\equiv dx^a/dt$. Se $I$ é dado por $a\leq t\leq b$, então (generalizando a equação \eqref{eq:ComprimentoCurvaVariedade}), nós podemos definir o \textit{comprimento} de $\gamma$ como sendo
\begin{equation}\label{eq:ComprimentoCurvaVariedade}
	\boxed{
	L = \int_a^b |g_{ab}\dot{x}^a\dot{x}^b|^{1/2}dt .
	}
\end{equation}
Se em uma curva o vetor tangente em cada ponto é nulo, de modo que $g_{ab}\dot{x}^a\dot{x}^b=0$ em todo ponto, temos uma curva de comprimento zero. Dizemos que esta é uma \textit{curva nula}.

Note que apenas definimos o comprimento de curvas, mas não a distância entre pares de pontos arbitrários em $M$. No entanto, podemos definir a distância $\delta s$ entre pontos próximos cujas diferenças nas coordenadas são pequenas. Eles podem ser considerados como pontos em uma curva dada por parâmetros cujas diferenças $\delta t$ são pequenas. Como, em primeira ordem, $\delta x^a= \dot{x}^a \delta t$, a definição nos dá $\delta s^2 = |g_{ab}\delta x^a\delta x^b|$. Na versão infinitesimal, temos
\begin{equation}\label{eq:ElementoDeLinhaVariedade}
	ds^2 = |g_{ab}dx^adx^b|
\end{equation}

Isso generaliza o conceito de comprimento Lorentziano, apresentado na seção \ref{sec:GeometriaHiperbolica}.
O tipo de variedade que utilizamos para modelar o espaço-tempo é uma variedade pseudo-riemanniana quadridimensional cujo tensor de métrica $g_{\mu\nu} (\mu,\nu=0,1,2,3)$ possui assinatura $(+---)$. Isso significa que  em qualquer ponto $P$ se nós adotarmos um sistema de coordenadas que nos dá $[g_{\mu\nu}]_P$ como uma matriz diagonal, então o primeiro elemento é positivo e os outros três são negativos. Todo vetor diferente de zero é, então, descrito como
\[
\left\{\begin{array}{l}{\text {do tipo tempo}} \\ {\text {nulo}} \\ {\text {do tipo espaço}}\end{array} \text { se } g_{\mu \nu} \lambda^{\mu} \lambda^{\nu}\left\{\begin{array}{l}{>0} \\ {=0} \\ {<0}\end{array}\right.\right. .
\]
Essas definições também valem para curvas (ou partes de curvas) a partir de seus vetores tangentes.